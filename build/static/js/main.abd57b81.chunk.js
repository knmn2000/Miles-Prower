(this["webpackJsonpsonic-templater"]=this["webpackJsonpsonic-templater"]||[]).push([[0],{36:function(e,t,n){e.exports=n(53)},45:function(e,t,n){},46:function(e,t,n){},53:function(e,t,n){"use strict";n.r(t);var s=n(32),r=n(10),a=n(12),l=n(34),o=n(0),i=n.n(o),c=n(16),p=n.n(c),m=(n(45),n(17)),u=n(2),d=(n(46),function(e){var t=e.text,n=e.link,s=Object(o.useRef)();return i.a.createElement("div",null,i.a.createElement("div",{className:"wrapper"},i.a.createElement("div",{role:"button",className:"retro-btn"},i.a.createElement(m.b,{to:{pathname:n},target:"/scraper"===n?null:"_blank",className:"btn",ref:s,onMouseMove:function(e){return function(e){var t=s.current.getBoundingClientRect().left,n=s.current.offsetWidth,r=e.pageX,a="";a=r<t+.3*n?"btn-left":r>t+.65*n?"btn-right":"btn-center";var l=s.current.className.replace(/btn-center|btn-right|btn-left/gi,"").trim();s.current.className=l+" "+a}(e)}},i.a.createElement("span",{className:"btn-inner"},i.a.createElement("span",{className:"content-wrapper"},i.a.createElement("span",{className:"btn-content"},i.a.createElement("span",{className:"btn-content-inner",label:t}))))))))}),_=function(){return console.log("knmn2000 says Hello"),i.a.createElement("div",{className:"landing"},i.a.createElement("span",{className:"landing-text"},"SONIC"),i.a.createElement("div",{className:"landing-buttons"},i.a.createElement(d,{text:"Check Sonic instance",link:"https://sonic.radicali.io/1/jobs/"}),i.a.createElement(d,{text:"Build a Scraper",link:"/scraper"})))},f=n(7),h=n(18),y=n.n(h),g=function(e){var t=e.text,n=Object(o.useRef)(),s=function(e){var t=n.current.className.replace(/btn-center|btn-right|btn-left/gi,"").trim();n.current.className=t+" btn-center"};return i.a.createElement("div",null,i.a.createElement("div",{className:"wrapper"},i.a.createElement("div",{role:"button",className:"retro-btn lg primary"},i.a.createElement("button",{className:"btn",ref:n,onMouseMove:function(e){return function(e){var t=n.current.getBoundingClientRect().left,s=n.current.offsetWidth,r=e.pageX,a="";a=r<t+.3*s?"btn-left":r>t+.65*s?"btn-right":"btn-center";var l=n.current.className.replace(/btn-center|btn-right|btn-left/gi,"").trim();n.current.className=l+" "+a}(e)},onClick:function(e){return s()},onMouseLeave:function(e){return s()}},i.a.createElement("span",{className:"btn-inner"},i.a.createElement("span",{className:"content-wrapper"},i.a.createElement("span",{className:"btn-content"},i.a.createElement("span",{className:"btn-content-inner",label:t}))))))))},E=n(35);function b(){var e=Object(E.a)(['import re\nimport time\nimport scrapy\n\nfrom sonic.settings import ItemType\nfrom sonic.sonic_utils import SonicUtils\n\n# BOTH SCRAPER AND TEST TEMPLATES AVAILABLE\nclass ScraperClass(scrapy.Spider):\n    # VERIFY NAMES AND ABBREVIATIONS\n    name = "X_NAME_ABBR"\n    regulator = "REG (CTRY)"\n    country = "COUNTRY"\n    regulator_full_name = "REGULATOR_FULL_NAME"\n    #### REGEX DATE PATTERNS -\n    pattern = re.compile(r"[d]{1,2} [ADFJMNOS]w* [d]{4}")\n    patternV2  = re.compile(r"[d]{1,2}/[d]{1,2}/[d]{1,2}")\n    patternV3 = re.compile(r"([12]d{3}.(0[1-9]|1[0-2]).(0[1-9]|[12]d|3[01]))")\n    patternV4 = re.compile(r"d{2}s[a-zA-Z]{3}sd{4}")\n    ####\n    main_url = "https://www.regulatorWebsite.com"\n    sheet_name = "X_NAME_ABBR"\n    fieldnames = [\n        "regulator",\n        "regulator_full_name",\n        "country",\n        "activity",\n        "document_type",\n        # match field names from zoho sheet\n    ]\n    target_urls = []\n    hardcoded_dates = {}  # possible ? getDates() : hardCodeDates()\n    utils = SonicUtils()\n    item_type = ItemType.REGULATION.value\n\n    def _parse_method_template(self, response):\n        """method to parse urls obtained from zoho and fetch \n           titles, links and dates\n        Args:\n            response (object): scrapy object containing response from the url\n\n        Yields:\n            [sonic.items.SonicItem] -- contains meta for each object scraped.\n        """\n        meta = response.meta\n        url = response.url\n        for element in elements:\n            link = element.xpath("//*LINK").extract_first()\n            title = element.xpath("//*TITLE").extract_first()\n            date = response.xpath("//*DATE").extract()\n            link = response.urljoin(link)\n            meta.update(\n                {\n                    "title": title,\n                    "link": link,\n                    "date": date,\n                    "response_url": response.url,\n                    "is_html": False,\n                }\n            )\n            yield self.utils.create_items(**meta)\n\n    def _parse_method_template_with_pagination(self, response):\n        """method to parse urls obtained from zoho and fetch \n           titles, links and dates\n        Args:\n            response (object): scrapy object containing response from the url\n\n        Yields:\n            [sonic.items.SonicItem] -- contains meta for each object scraped.\n        """\n        meta = response.meta\n        url = response.url\n        for element in elements:\n            link = element.xpath("//*LINK").extract_first()\n            title = element.xpath("//*TITLE").extract_first()\n            date = response.xpath("//*DATE").extract()\n            link = response.urljoin(link)\n            meta.update(\n                {\n                    "title": title,\n                    "link": link,\n                    "date": date,\n                    "response_url": response.url,\n                    "is_html": False,\n                }\n            )\n            yield self.utils.create_items(**meta)\n\n        next_page = response.xpath("//*PAGINATION_LINK").extract_first()\n        if next_page is not None:  # Change condition as required\n            next_page_url = response.urljoin(next_page)\n            yield scrapy.Request(\n                next_page_url, callback=self._parse_consultation, meta=meta\n            )\n\n    # VERSION 1 HTML CONTENT METHOD\n    def _parse_html_content(self, response):\n        self.driver.get(response.url)\n        WebDriverWait(self.driver, self.delay).until(\n            EC.presence_of_element_located((By.XPATH, "//RELEVANT XPATH"))\n        )\n        response = HtmlResponse(\n            self.driver.current_url, body=self.driver.page_source, encoding="utf-8"\n        )\n        title = element.xpath("//*TITLE").extract_first()\n        date = response.xpath("//*DATE").extract()\n        html_content = " ".join(response.xpath("//XPATH TO TEXT CONTENTS").extract())\n        html_meta.update(\n            {\n                "title": title,\n                "html_content": html_content,\n                "is_html": True,\n                "link": response.url,\n                "date": date,\n                "ignore_link_check": True,\n            }\n        )\n        yield self.utils.create_items(**html_meta)\n\n    def start_requests(self):\n        """Scrapy method to start requests, call parsers and pass meta info based on the urls fetched from zoho.\n\n        Yields:\n            scrapy.Request: scrapy Request constructor to complete the download and pass in the parsers in callback\n        """\n        urls, meta_list = self.utils.read_from_zoho(\n            country_from_sheet_flag=True,\n            sheet_name=self.sheet_name,\n            fieldnames=self.fieldnames,\n            regulator_full_name=self.regulator_full_name,\n            regulator=self.regulator,\n            country=self.country,\n        )\n\n        for url, meta in zip(urls, meta_list):\n            meta.update({"item_type": self.item_type})\n            document_type = meta.get("document_type")\n            activity = meta.get("activity")\n            if document_type == "doc_type":\n                yield scrapy.Request(\n                    url, callback=self._parse_method_template, meta=meta\n                )\n            if activity == "activity":\n                yield scrapy.Request(\n                    url, callback=self._parse_method_template, meta=meta\n                )\n            if "keyword" in url:\n                yield scrapy.Request(\n                    url, callback=self._parse_method_template, meta=meta\n                )\n            if url in self.target_urls:\n                yield scrapy.Request(\n                    url, callback=self._parse_method_template, meta=meta\n                )\n##########################################################\n# TEMPLATE FOR TESTS \nimport logging\nimport time\nimport unittest\n\nimport pytest\nimport requests\nimport vcr\nfrom scrapy.http import HtmlResponse, Request\n\nfrom sonic.pipelines import SonicPipeline\nfrom sonic.settings import ItemType\nfrom sonic.spiders.regulation_spiders import ScraperClass \n\n# @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\nclass TestScraperClass:\n    spider = ScraperClass()\n    example_url = "https://www.dummy.com"\n    another_url = "https://www.dummy.com"\n    other_url = "https://www.dummy.com"\n    list_links = ["https://www.dummy.com", "https://www.dummy.com"]\n\n    start_urls_list = [\n        example_url,\n        other_url,\n        another_url,\n    ]\n    parse_list = [\n        example_url,\n        other_url,\n        another_url,\n    ]\n    meta = {\n        "item_type": ItemType.REGULATION.value,\n        "regulator": "SC (MY)",\n    }\n    spider.wait_time = 0\n    pipeline = SonicPipeline()\n    # TEST METHOD TEMPLATES\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    # @pytest.mark.slow\n    # @pytest.mark.vcr\n    def test_parse_method_name(self):\n        response = requests.get(self.example_url)\n        request = Request(url=self.example_url, meta=self.meta,)\n        response = HtmlResponse(\n            url=self.example_url,\n            request=request,\n            body=response.content,\n            encoding="utf-8",\n        )\n        gen = self.spider._parse_example(response)\n        results = []\n        for item in gen:\n            self.pipeline.process_item(item, self.spider)\n            results.append(item)\n        assert len(results) == 1\n\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    # @pytest.mark.slow\n    # @pytest.mark.vcr\n    def test_parse_method_other_name(self, requests_session, mock_time):\n        results = []\n        for url in self.list_of_links:\n            response = requests_session.get(url)\n            request = Request(url=url, meta=self.meta)\n            response = HtmlResponse(\n                url=url, request=request, body=response.content, encoding="utf-8",\n            )\n            gen = self.spider._relevant_method(response)\n            for item in gen:\n                self.pipeline.process_item(item, self.spider)\n                results.append(item)\n        assert len(results) == 1\n\n    # IF METHOD HAS BOTH PAYLOAD YIELDS AND REQUEST YIELDS **\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    # @pytest.mark.slow\n    # @pytest.mark.vcr\n    def test_parse_method(self, requests_session, mock_time):\n        results = []\n        for url in self.list_links:\n            response = requests_session.get(url)\n            request = Request(url=url, meta=self.meta)\n            response = HtmlResponse(\n                url=url, request=request, body=response.content, encoding="utf-8",\n            )\n            gen = self.spider._parse_papers(response)\n            for item in gen:\n                try:\n                    self.pipeline.process_item(item, self.spider)\n                except AttributeError:\n                    pass\n                results.append(item)\n        assert len(results) == 1\n\n    # IF METHOD HAS "ENTRIES" ARGUMENT\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    # @pytest.mark.slow\n    # @pytest.mark.vcr\n    def test_parse_pdf_entries(self, requests_session, mock_time):\n        results = []\n        response = requests_session.get(self.example_url)\n        request = Request(url=self.example_url, meta=self.meta)\n        response = HtmlResponse(\n            url=self.example_url,\n            request=request,\n            body=response.content,\n            encoding="utf-8",\n        )\n        entries = response.xpath("//XPATH FOR ENTRIES")\n        gen = self.spider._parse_method(response, entries)\n        for request in gen:\n            self.pipeline.process_item(request, self.spider)\n            results.append(request)\n        assert len(results) == 1\n\n    # PARSE TEST METHODS\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    def test_parse(self, monkeypatch, requests_session):\n        metas = []\n        url_list = self.parse_list\n        for url in url_list:\n            if url == self.consultation_url:\n                meta.update({"document_type": "Consultation Paper"})\n            else:\n                meta.update({"document_type": "dummy"})\n            response = requests_session.get(url)\n            request = Request(url=url, meta=self.meta)\n            response = HtmlResponse(\n                url=url, request=request, body=response.content, encoding="utf-8",\n            )\n            if url == self.consultation_url:\n                feedback = "parse consultation called"\n                monkeypatch.setattr(\n                    self.spider, "_parse_consultations", lambda s: feedback,\n                )\n            if "something" in url:\n                feedback = "parse something called"\n                monkeypatch.setattr(\n                    self.spider,\n                    "_parse_something",\n                    lambda s, entries: feedback,\n                    "dummy",\n                )\n            if url == self.something_else:\n                feedback = "_something_else called"\n                monkeypatch.setattr(self.spider, "_something_else", lambda s: feedback)\n            result = self.spider.parse(response)\n            if type(result) == str:\n                assert result == feedback\n            else:\n                metas.append([i for i in result])\n\n    # START REQUEST TEST METHODS\n    def test_start_requests(self, monkeypatch):\n        for url in self.start_urls_list:\n            monkeypatch.setattr(\n                self.spider.utils, "read_from_zoho", lambda **args: ([url], [self.meta])\n            )\n            gen = self.spider.start_requests()\n            for response in gen:\n                if url == self.example_url:\n                    assert response.callback.__name__ == "_parse_this_url"\n                if "/something" in url:\n                    assert response.callback.__name__ == "_parse_that_url"\n\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    def test_start_requests(self, monkeypatch):\n        response = []\n        for url in self.start_urls_list:\n            monkeypatch.setattr(\n                self.spider.utils,\n                "read_from_zoho",\n                lambda **kwargs: ([url], self.meta),\n            )\n            gen = self.spider.start_requests()\n            response.append([i for i in gen])\n        assert len(response) >= 1\n'],['import re\nimport time\nimport scrapy\n\nfrom sonic.settings import ItemType\nfrom sonic.sonic_utils import SonicUtils\n\n# BOTH SCRAPER AND TEST TEMPLATES AVAILABLE\nclass ScraperClass(scrapy.Spider):\n    # VERIFY NAMES AND ABBREVIATIONS\n    name = "X_NAME_ABBR"\n    regulator = "REG (CTRY)"\n    country = "COUNTRY"\n    regulator_full_name = "REGULATOR_FULL_NAME"\n    #### REGEX DATE PATTERNS -\n    pattern = re.compile(r"[\\d]{1,2} [ADFJMNOS]\\w* [\\d]{4}")\n    patternV2  = re.compile(r"[\\d]{1,2}\\/[\\d]{1,2}\\/[\\d]{1,2}")\n    patternV3 = re.compile(r"([12]\\d{3}.(0[1-9]|1[0-2]).(0[1-9]|[12]\\d|3[01]))")\n    patternV4 = re.compile(r"\\d{2}\\s[a-zA-Z]{3}\\s\\d{4}")\n    ####\n    main_url = "https://www.regulatorWebsite.com"\n    sheet_name = "X_NAME_ABBR"\n    fieldnames = [\n        "regulator",\n        "regulator_full_name",\n        "country",\n        "activity",\n        "document_type",\n        # match field names from zoho sheet\n    ]\n    target_urls = []\n    hardcoded_dates = {}  # possible ? getDates() : hardCodeDates()\n    utils = SonicUtils()\n    item_type = ItemType.REGULATION.value\n\n    def _parse_method_template(self, response):\n        """method to parse urls obtained from zoho and fetch \n           titles, links and dates\n        Args:\n            response (object): scrapy object containing response from the url\n\n        Yields:\n            [sonic.items.SonicItem] -- contains meta for each object scraped.\n        """\n        meta = response.meta\n        url = response.url\n        for element in elements:\n            link = element.xpath("//*LINK").extract_first()\n            title = element.xpath("//*TITLE").extract_first()\n            date = response.xpath("//*DATE").extract()\n            link = response.urljoin(link)\n            meta.update(\n                {\n                    "title": title,\n                    "link": link,\n                    "date": date,\n                    "response_url": response.url,\n                    "is_html": False,\n                }\n            )\n            yield self.utils.create_items(**meta)\n\n    def _parse_method_template_with_pagination(self, response):\n        """method to parse urls obtained from zoho and fetch \n           titles, links and dates\n        Args:\n            response (object): scrapy object containing response from the url\n\n        Yields:\n            [sonic.items.SonicItem] -- contains meta for each object scraped.\n        """\n        meta = response.meta\n        url = response.url\n        for element in elements:\n            link = element.xpath("//*LINK").extract_first()\n            title = element.xpath("//*TITLE").extract_first()\n            date = response.xpath("//*DATE").extract()\n            link = response.urljoin(link)\n            meta.update(\n                {\n                    "title": title,\n                    "link": link,\n                    "date": date,\n                    "response_url": response.url,\n                    "is_html": False,\n                }\n            )\n            yield self.utils.create_items(**meta)\n\n        next_page = response.xpath("//*PAGINATION_LINK").extract_first()\n        if next_page is not None:  # Change condition as required\n            next_page_url = response.urljoin(next_page)\n            yield scrapy.Request(\n                next_page_url, callback=self._parse_consultation, meta=meta\n            )\n\n    # VERSION 1 HTML CONTENT METHOD\n    def _parse_html_content(self, response):\n        self.driver.get(response.url)\n        WebDriverWait(self.driver, self.delay).until(\n            EC.presence_of_element_located((By.XPATH, "//RELEVANT XPATH"))\n        )\n        response = HtmlResponse(\n            self.driver.current_url, body=self.driver.page_source, encoding="utf-8"\n        )\n        title = element.xpath("//*TITLE").extract_first()\n        date = response.xpath("//*DATE").extract()\n        html_content = " ".join(response.xpath("//XPATH TO TEXT CONTENTS").extract())\n        html_meta.update(\n            {\n                "title": title,\n                "html_content": html_content,\n                "is_html": True,\n                "link": response.url,\n                "date": date,\n                "ignore_link_check": True,\n            }\n        )\n        yield self.utils.create_items(**html_meta)\n\n    def start_requests(self):\n        """Scrapy method to start requests, call parsers and pass meta info based on the urls fetched from zoho.\n\n        Yields:\n            scrapy.Request: scrapy Request constructor to complete the download and pass in the parsers in callback\n        """\n        urls, meta_list = self.utils.read_from_zoho(\n            country_from_sheet_flag=True,\n            sheet_name=self.sheet_name,\n            fieldnames=self.fieldnames,\n            regulator_full_name=self.regulator_full_name,\n            regulator=self.regulator,\n            country=self.country,\n        )\n\n        for url, meta in zip(urls, meta_list):\n            meta.update({"item_type": self.item_type})\n            document_type = meta.get("document_type")\n            activity = meta.get("activity")\n            if document_type == "doc_type":\n                yield scrapy.Request(\n                    url, callback=self._parse_method_template, meta=meta\n                )\n            if activity == "activity":\n                yield scrapy.Request(\n                    url, callback=self._parse_method_template, meta=meta\n                )\n            if "keyword" in url:\n                yield scrapy.Request(\n                    url, callback=self._parse_method_template, meta=meta\n                )\n            if url in self.target_urls:\n                yield scrapy.Request(\n                    url, callback=self._parse_method_template, meta=meta\n                )\n##########################################################\n# TEMPLATE FOR TESTS \nimport logging\nimport time\nimport unittest\n\nimport pytest\nimport requests\nimport vcr\nfrom scrapy.http import HtmlResponse, Request\n\nfrom sonic.pipelines import SonicPipeline\nfrom sonic.settings import ItemType\nfrom sonic.spiders.regulation_spiders import ScraperClass \n\n# @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\nclass TestScraperClass:\n    spider = ScraperClass()\n    example_url = "https://www.dummy.com"\n    another_url = "https://www.dummy.com"\n    other_url = "https://www.dummy.com"\n    list_links = ["https://www.dummy.com", "https://www.dummy.com"]\n\n    start_urls_list = [\n        example_url,\n        other_url,\n        another_url,\n    ]\n    parse_list = [\n        example_url,\n        other_url,\n        another_url,\n    ]\n    meta = {\n        "item_type": ItemType.REGULATION.value,\n        "regulator": "SC (MY)",\n    }\n    spider.wait_time = 0\n    pipeline = SonicPipeline()\n    # TEST METHOD TEMPLATES\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    # @pytest.mark.slow\n    # @pytest.mark.vcr\n    def test_parse_method_name(self):\n        response = requests.get(self.example_url)\n        request = Request(url=self.example_url, meta=self.meta,)\n        response = HtmlResponse(\n            url=self.example_url,\n            request=request,\n            body=response.content,\n            encoding="utf-8",\n        )\n        gen = self.spider._parse_example(response)\n        results = []\n        for item in gen:\n            self.pipeline.process_item(item, self.spider)\n            results.append(item)\n        assert len(results) == 1\n\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    # @pytest.mark.slow\n    # @pytest.mark.vcr\n    def test_parse_method_other_name(self, requests_session, mock_time):\n        results = []\n        for url in self.list_of_links:\n            response = requests_session.get(url)\n            request = Request(url=url, meta=self.meta)\n            response = HtmlResponse(\n                url=url, request=request, body=response.content, encoding="utf-8",\n            )\n            gen = self.spider._relevant_method(response)\n            for item in gen:\n                self.pipeline.process_item(item, self.spider)\n                results.append(item)\n        assert len(results) == 1\n\n    # IF METHOD HAS BOTH PAYLOAD YIELDS AND REQUEST YIELDS **\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    # @pytest.mark.slow\n    # @pytest.mark.vcr\n    def test_parse_method(self, requests_session, mock_time):\n        results = []\n        for url in self.list_links:\n            response = requests_session.get(url)\n            request = Request(url=url, meta=self.meta)\n            response = HtmlResponse(\n                url=url, request=request, body=response.content, encoding="utf-8",\n            )\n            gen = self.spider._parse_papers(response)\n            for item in gen:\n                try:\n                    self.pipeline.process_item(item, self.spider)\n                except AttributeError:\n                    pass\n                results.append(item)\n        assert len(results) == 1\n\n    # IF METHOD HAS "ENTRIES" ARGUMENT\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    # @pytest.mark.slow\n    # @pytest.mark.vcr\n    def test_parse_pdf_entries(self, requests_session, mock_time):\n        results = []\n        response = requests_session.get(self.example_url)\n        request = Request(url=self.example_url, meta=self.meta)\n        response = HtmlResponse(\n            url=self.example_url,\n            request=request,\n            body=response.content,\n            encoding="utf-8",\n        )\n        entries = response.xpath("//XPATH FOR ENTRIES")\n        gen = self.spider._parse_method(response, entries)\n        for request in gen:\n            self.pipeline.process_item(request, self.spider)\n            results.append(request)\n        assert len(results) == 1\n\n    # PARSE TEST METHODS\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    def test_parse(self, monkeypatch, requests_session):\n        metas = []\n        url_list = self.parse_list\n        for url in url_list:\n            if url == self.consultation_url:\n                meta.update({"document_type": "Consultation Paper"})\n            else:\n                meta.update({"document_type": "dummy"})\n            response = requests_session.get(url)\n            request = Request(url=url, meta=self.meta)\n            response = HtmlResponse(\n                url=url, request=request, body=response.content, encoding="utf-8",\n            )\n            if url == self.consultation_url:\n                feedback = "parse consultation called"\n                monkeypatch.setattr(\n                    self.spider, "_parse_consultations", lambda s: feedback,\n                )\n            if "something" in url:\n                feedback = "parse something called"\n                monkeypatch.setattr(\n                    self.spider,\n                    "_parse_something",\n                    lambda s, entries: feedback,\n                    "dummy",\n                )\n            if url == self.something_else:\n                feedback = "_something_else called"\n                monkeypatch.setattr(self.spider, "_something_else", lambda s: feedback)\n            result = self.spider.parse(response)\n            if type(result) == str:\n                assert result == feedback\n            else:\n                metas.append([i for i in result])\n\n    # START REQUEST TEST METHODS\n    def test_start_requests(self, monkeypatch):\n        for url in self.start_urls_list:\n            monkeypatch.setattr(\n                self.spider.utils, "read_from_zoho", lambda **args: ([url], [self.meta])\n            )\n            gen = self.spider.start_requests()\n            for response in gen:\n                if url == self.example_url:\n                    assert response.callback.__name__ == "_parse_this_url"\n                if "/something" in url:\n                    assert response.callback.__name__ == "_parse_that_url"\n\n    # @pytest.mark.vcr(match_on=["method", "scheme", "host", "query"])\n    def test_start_requests(self, monkeypatch):\n        response = []\n        for url in self.start_urls_list:\n            monkeypatch.setattr(\n                self.spider.utils,\n                "read_from_zoho",\n                lambda **kwargs: ([url], self.meta),\n            )\n            gen = self.spider.start_requests()\n            response.append([i for i in gen])\n        assert len(response) >= 1\n']);return b=function(){return e},e}var T={template:String.raw(b()),selenium_imports:'import logging\nimport re\nimport time\nfrom os.path import abspath, dirname, join\n\nimport dateparser\nimport scrapy\nfrom scrapy.http import HtmlResponse\nfrom selenium import webdriver\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.firefox.options import Options\nfrom selenium.webdriver.remote.remote_connection import LOGGER\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import Select, WebDriverWait\n\nfrom sonic.settings import ItemType\nfrom sonic.sonic_utils import SonicUtils\n\nLOGGER.setLevel(logging.ERROR)\n\nPATH_DIR = dirname(abspath(__file__))\nGECKODRIVER_PATH = abspath(join(PATH_DIR, "../../geckodriver"))\n\n#### DEFINE CLASS NAME AND VARIABLES \n\ndef __init__(self, *args, **kwargs):\n        options = Options()\n        options.headless = True\n        self.driver = webdriver.Firefox(\n            options=options, executable_path=GECKODRIVER_PATH\n        )\n\ndef __del__(self):\n    self.driver.quit()\n\n#### INSIDE PARSER METHOD \nmeta = response.meta\nsource = response.url\nself.driver.get(source)\n',pagination:"\nnext_page = response.xpath('//LINK TO NEXT PAGE').extract_first()\nif next_page is not None:\nnext_page_url = response.urljoin(next_page)\nyield scrapy.Request(next_page_url, callback=self._parse_method, meta=meta)\n\n###### VER 2\n\n# FROM HTML_SGX_SG\nnext_page = response.xpath('//LINK TO NEXT PAGE')\nlast_page = next_page.xpath(\"./CONDITION FOR LAST PAGE\").extract_first()\nif last_page is None:\n    next_page_url = next_page.xpath(\"./@href\").extract_first()\n    if next_page_url is not None:\n        next_page_url = response.urljoin(next_page_url)\n        yield scrapy.Request(\n            next_page_url, callback=self._parse_method, meta=meta\n        )\n\n###### VER 3\n\n# FROM N_BAFIN_GE\nnext_page = response.xpath(\n            '//*NEXT PAGE LINK'\n        ).extract_first()\nif next_page is not None:\n    next_page_url = response.urljoin(next_page)\n    yield scrapy.Request(next_page_url, callback=self._parse_method, meta=meta)",infinite_scrolling:'  \n# FROM N_ECB_EU\n\nmeta = response.meta\nself.driver.get(response.url)\nself.driver.implicitly_wait(self.wait_time)\nbody = self.driver.page_source\nactions = ActionChains(self.driver)\nresponse = HtmlResponse(\n    self.driver.current_url, body=body, encoding="utf-8"\n)  # handing over html response from selenium\nsnippets = self.driver.find_elements_by_xpath("//*ELEMENTS THAT LOAD ON SCROLLING")\nfor snippet in snippets:\n    # loading parts of the page by scrolling them into view\n    self.driver.execute_script("arguments[0].scrollIntoView();", snippet)\n    entries = snippet.find_elements_by_xpath("./FIND NEXT ELEMENT")\n    for entry in entries:\n        try:\n            # scraping code \n        except:\n            self.driver.implicitly_wait(self.wait_time)',html_response:'\nmeta = response.meta\nself.driver.get(response.url)\nself.driver.implicitly_wait(self.wait_time)\nbody = self.driver.page_source\nactions = ActionChains(self.driver)\nresponse = HtmlResponse(\n    self.driver.current_url, body=body, encoding="utf-8"\n)  # handing over html response from selenium\n\n#### With Explicit wait\nWebDriverWait(self.driver, self.delay).until(\n     EC.presence_of_element_located((By.XPATH, \'Xpath to be found\'))\n)\nbody = self.driver.page_source\nresponse = HtmlResponse(\n    self.driver.current_url, body=body, encoding="utf-8"\n)'},R=(n(29),n(30),n(31),function(e){var t=e.text,n=Object(o.useRef)(),s=function(e){var t=n.current.className.replace(/btn-center|btn-right|btn-left/gi,"").trim();n.current.className=t+" btn-center"};return i.a.createElement("div",null,i.a.createElement("div",{className:"wrapper"},i.a.createElement("div",{role:"button",className:"retro-btn sm primary"},i.a.createElement("button",{className:"btn",style:{width:"95%"},ref:n,onMouseMove:function(e){return function(e){var t=n.current.getBoundingClientRect().left,s=n.current.offsetWidth,r=e.pageX,a="";a=r<t+.3*s?"btn-left":r>t+.65*s?"btn-right":"btn-center";var l=n.current.className.replace(/btn-center|btn-right|btn-left/gi,"").trim();n.current.className=l+" "+a}(e)},onClick:function(e){return s()},onMouseLeave:function(e){return s()}},i.a.createElement("span",{className:"btn-inner"},i.a.createElement("span",{className:"content-wrapper"},i.a.createElement("span",{className:"btn-content"},i.a.createElement("span",{className:"btn-content-inner",label:t}))))))))}),v=Object(a.b)((function(e){return{details:e.scraper.details}}),{editName:function(e){return function(t){try{t({type:"EDIT_NAME",payload:e})}catch(n){console.log(n),t({type:"ERROR",payload:n})}}},editClass:function(e){return function(t){try{t({type:"EDIT_CLASS",payload:e})}catch(n){console.log(n),t({type:"ERROR",payload:n})}}},editCountry:function(e){return function(t){try{t({type:"EDIT_COUNTRY",payload:e})}catch(n){console.log(n),t({type:"ERROR",payload:n})}}},editRegulator:function(e){return function(t){try{t({type:"EDIT_REG",payload:e})}catch(n){console.log(n),t({type:"ERROR",payload:n})}}},editRegFull:function(e){return function(t){try{t({type:"EDIT_REG_FULL",payload:e})}catch(n){console.log(n),t({type:"ERROR",payload:n})}}},editUrl:function(e){return function(t){try{t({type:"EDIT_URL",payload:e})}catch(n){console.log(n),t({type:"ERROR",payload:n})}}}})((function(e){var t=e.open,n=e.editName,s=e.editClass,r=e.editCountry,a=e.editRegulator,l=e.editRegFull,c=e.editUrl,p=e.details,m=Object(o.useState)("#Template Code here"),u=Object(f.a)(m,2),d=u[0],_=u[1],h=Object(o.useState)(p.name),g=Object(f.a)(h,2),E=g[0],b=g[1],v=Object(o.useState)(p.regulator),N=Object(f.a)(v,2),k=N[0],x=N[1],A=Object(o.useState)(p.country),S=Object(f.a)(A,2),O=S[0],q=S[1],w=Object(o.useState)(p.regulator_full_name),I=Object(f.a)(w,2),j=I[0],L=I[1],C=Object(o.useState)(p.main_url),D=Object(f.a)(C,2),M=D[0],H=D[1],P=Object(o.useState)(p.class_name),B=Object(f.a)(P,2),G=B[0],U=B[1];return Object(o.useEffect)((function(){b(E),x(k),q(O),L(j),H(M),U(G)}),[p,E,G,M,j,O,k]),i.a.createElement(i.a.Fragment,null,i.a.createElement("div",{className:t?"slider slider-slide":"slider"},i.a.createElement("div",{className:"sidebar"},i.a.createElement("div",{className:"sidebar-one"},i.a.createElement("form",{className:"details"},i.a.createElement("h1",{className:"form-header"},"Scraper details"),i.a.createElement("div",{className:"form-grid"},i.a.createElement("div",{className:"grid-one"},i.a.createElement("input",{name:"name",type:"text",placeholder:"Name [R_ABC_XY1]",className:"input-field",onChange:function(e){var t;b(e.target.value),t={name:e.target.value,regulator:k,class_name:G,regulator_full_name:j,main_url:M,country:O},n(t)}}),i.a.createElement("input",{name:"country",type:"text",placeholder:"Country",className:"input-field",onChange:function(e){var t;q(e.target.value),t={name:E,regulator:k,class_name:G,regulator_full_name:j,main_url:M,country:e.target.value},r(t)}}),i.a.createElement("input",{name:"regulator_full_name",type:"text",placeholder:"Regulator full name",className:"input-field",onChange:function(e){var t;L(e.target.value),t={name:E,regulator:k,class_name:G,regulator_full_name:e.target.value,main_url:M,country:O},l(t)}})),i.a.createElement("div",{className:"grid-two"},i.a.createElement("input",{name:"main url",type:"text",placeholder:"Main URL",className:"input-field",onChange:function(e){var t;H(e.target.value),t={name:E,regulator:k,class_name:G,regulator_full_name:j,main_url:e.target.value,country:O},c(t)}}),i.a.createElement("input",{name:"reg",type:"text",placeholder:"regulator [REG (CTRY)]",className:"input-field",onChange:function(e){var t;x(e.target.value),t={name:E,regulator:e.target.value,class_name:G,regulator_full_name:j,main_url:M,country:O},a(t)}}),i.a.createElement("input",{name:"classname",type:"text",placeholder:"Class name",className:"input-field",onChange:function(e){var t;U(e.target.value),t={name:E,regulator:k,class_name:e.target.value,regulator_full_name:j,main_url:M,country:O},s(t)}}))))),i.a.createElement("div",{className:"sidebar-two"},i.a.createElement("div",{className:"form-buttons"},i.a.createElement("div",{onClick:function(){return _(T.selenium_imports)}},i.a.createElement(R,{text:"selenium imports"})),i.a.createElement("div",{onClick:function(){return _(T.pagination)}},i.a.createElement(R,{text:"pagination"})),i.a.createElement("div",{onClick:function(){return _(T.infinite_scrolling)}},i.a.createElement(R,{text:"infinite scrolling"})),i.a.createElement("div",{onClick:function(){return _(T.html_response)}},i.a.createElement(R,{text:"HTML response"}))),i.a.createElement(y.a,{style:{height:"45vh",width:"100%"},fontSize:14,mode:"python",theme:"monokai",onChange:function(e){console.log("change",e)},wrapEnabled:!0,name:"SonicScraper",value:d,editorProps:{$blockScrolling:!0},setOptions:{enableBasicAutocompletion:!0,enableLiveAutocompletion:!0,enableSnippets:!0}})))))})),N=Object(a.b)((function(e){return{template:e.scraper.template,type:e.scraper.type}}),{editTemplate:function(e){return function(t){try{t({type:"EDIT_TEMPLATE",payload:e})}catch(n){console.log(n),t({type:"ERROR",payload:n})}}},editType:function(e){return function(t){try{t({type:"EDIT TYPE",payload:e})}catch(n){console.log(n),t({type:"ERROR",payload:n})}}}})((function(e){var t=e.template,n=e.editTemplate,s=e.editType,r=e.type;var a=Object(o.useState)(t),l=Object(f.a)(a,2),c=l[0],p=l[1],m=Object(o.useState)(!1),u=Object(f.a)(m,2),d=u[0],_=u[1],h=Object(o.useState)(!1),E=Object(f.a)(h,2),b=E[0],T=E[1];function R(e){"news"===e?_(!d):"reg"===e&&T(!b),s(e)}return Object(o.useEffect)((function(){t="news"===r?t.split("ItemType.REGULATION").join("ItemType.NEWS").split("sonic.spiders.regulation_spiders").join("sonic.spiders.news_spiders"):t.split("ItemType.NEWS").join("ItemType.REGULATION").split("sonic.spiders.news_spiders").join("sonic.spiders.regulation_spiders"),p(t),n(t)}),[p,n,r,R]),i.a.createElement("div",{className:"editor"},i.a.createElement(y.a,{style:{height:"100vh",width:"40vw",zIndex:"999"},fontSize:12,mode:"python",theme:"monokai",onChange:function(e){console.log("change",e)},wrapEnabled:!0,name:"SonicScraper",value:c,editorProps:{$blockScrolling:!0},setOptions:{enableBasicAutocompletion:!0,enableLiveAutocompletion:!0,enableSnippets:!0}}),i.a.createElement("div",{className:b||d?"customize customize-slide":"customize"},i.a.createElement("div",{className:"group"},i.a.createElement("div",{className:b?"scraperBtn invisible":"scraperBtn",onClick:function(){b||R("news")}},i.a.createElement(g,{text:"News Scraper"})),i.a.createElement("div",{className:d?"scraperBtn invisible":"scraperBtn",onClick:function(){d||R("reg")}},i.a.createElement(g,{text:"Reg. Scraper"})))),i.a.createElement(v,{open:!(!b&&!d)}))}));var k=function(){return i.a.createElement(m.a,null,i.a.createElement("div",{className:"App"},i.a.createElement(u.c,null,i.a.createElement(u.a,{exact:!0,path:"/",component:_}),i.a.createElement(u.a,{exact:!0,path:"/scraper",component:N}))))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));var x=n(3),A={template:T.template,details:{name:"X_NAME_ABBR",regulator:"REG (CTRY)",country:"COUNTRY",regulator_full_name:"REGULATOR_FULL_NAME",main_url:"https://www.regulatorWebsite.com",class_name:"ScraperClass"},type:"reg",loading:null,error:null},S=null,O=Object(r.combineReducers)({scraper:function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:A,t=arguments.length>1?arguments[1]:void 0;switch(t.type){case"EDIT TYPE":return Object(x.a)(Object(x.a)({},e),{},{loading:!1,type:t.payload});case"EDIT_TEMPLATE":return Object(x.a)(Object(x.a)({},e),{},{loading:!1,template:t.payload});case"EDIT_NAME":return S=e.template.split(' name = "'.concat(e.details.name)).join(' name = "'.concat(t.payload.name)).split(' sheet_name = "'.concat(e.details.name)).join(' sheet_name = "'.concat(t.payload.name)),Object(x.a)(Object(x.a)({},e),{},{template:S,loading:!1,details:t.payload});case"EDIT_CLASS":return S=e.template.split("class ".concat(e.details.class_name,"(scrapy.Spider):")).join("class ".concat(t.payload.class_name,"(scrapy.Spider):")).split("sonic.spiders.regulation_spiders import ".concat(e.details.class_name)).join("sonic.spiders.regulation_spiders import ".concat(t.payload.class_name)).split("sonic.spiders.news_spiders import ".concat(e.details.class_name)).join("sonic.spiders.news_spiders import ".concat(t.payload.class_name)).split("Test".concat(e.details.class_name)).join("Test".concat(t.payload.class_name)).split("spider = ".concat(e.details.class_name)).join("spider = ".concat(t.payload.class_name)),Object(x.a)(Object(x.a)({},e),{},{template:S,loading:!1,details:t.payload});case"EDIT_COUNTRY":return S=e.template.split('country = "'.concat(e.details.country,'"')).join('country = "'.concat(t.payload.country,'"')),Object(x.a)(Object(x.a)({},e),{},{template:S,loading:!1,details:t.payload});case"EDIT_REG":return S=e.template.split('regulator = "'.concat(e.details.regulator,'"')).join('regulator = "'.concat(t.payload.regulator,'"')),Object(x.a)(Object(x.a)({},e),{},{template:S,loading:!1,details:t.payload});case"EDIT_REG_FULL":return S=e.template.split('regulator_full_name = "'.concat(e.details.regulator_full_name,'"')).join('regulator_full_name = "'.concat(t.payload.regulator_full_name,'"')),Object(x.a)(Object(x.a)({},e),{},{template:S,loading:!1,details:t.payload});case"EDIT_URL":return S=e.template.split('main_url = "'.concat(e.details.main_url,'"')).join('main_url = "'.concat(t.payload.main_url,'"')),Object(x.a)(Object(x.a)({},e),{},{template:S,loading:!1,details:t.payload});case"ERROR":default:return e}}}),q=[l.a],w=Object(r.createStore)(O,{},Object(s.composeWithDevTools)(r.applyMiddleware.apply(void 0,q)));p.a.render(i.a.createElement(i.a.StrictMode,null,i.a.createElement(a.a,{store:w},i.a.createElement(k,null))),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()})).catch((function(e){console.error(e.message)}))}},[[36,1,2]]]);
//# sourceMappingURL=main.abd57b81.chunk.js.map